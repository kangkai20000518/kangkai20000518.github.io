---
layout: post
title: Image Classification
---

## **ยง1. Load Packages and Obtain Data**
By running this code, we have created TensorFlow Datasets for training, validation, and testing. 


```python
import os
from tensorflow.keras import utils 
import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt
import numpy as np

# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```

    Downloading data from https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip
    68608000/68606236 [==============================] - 1s 0us/step
    68616192/68606236 [==============================] - 1s 0us/step
    Found 2000 files belonging to 2 classes.
    Found 1000 files belonging to 2 classes.
    

I creat a funciton that In the first row, show three random pictures of cats. In the second row, show three random pictures of dogs. 


```python
def two_row_visualization(train_dataset):
  class_names = train_dataset.class_names

  plt.figure(figsize=(10, 10))
  for images, labels in train_dataset.take(1):
    cat=images[labels==0]
    dog=images[labels==1]
    for i in range(6):
      ax = plt.subplot(3, 3, i + 1)
      if i<=2:
        plt.imshow(cat[i].numpy().astype("uint8"))
        plt.title("cat")
        plt.axis("off")
      else:
        plt.imshow(dog[i].numpy().astype("uint8"))
        plt.title("dog")
        plt.axis("off")
```


```python
two_row_visualization(train_dataset)
```


![output_4_0.png]({{ site.baseurl }}/images/output_4_0.png)    

    


This is technical code related to rapidly reading data. 


```python
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```

The following line of code will create an iterator called labels.


```python
labels_iterator= train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()
```

## **ยง2. First Model**


```python
model1 = models.Sequential([
      layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
      layers.MaxPooling2D((2, 2)),
      layers.Conv2D(32, (3, 3), activation='relu'),
      layers.MaxPooling2D((2, 2)),
      layers.Dropout(0.2),
      layers.Flatten(), # n^2 * 64 length vector
      
      layers.Dense(2) # number of classes in your dataset
])
```


```python
model1.compile(optimizer='adam', 
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics = ['accuracy'])
```


```python
history = model1.fit(train_dataset, 
           epochs=20, 
           validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 6s 87ms/step - loss: 29.0706 - accuracy: 0.5485 - val_loss: 0.7319 - val_accuracy: 0.5384
    Epoch 2/20
    63/63 [==============================] - 6s 83ms/step - loss: 0.6417 - accuracy: 0.6065 - val_loss: 0.7231 - val_accuracy: 0.5668
    Epoch 3/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.5454 - accuracy: 0.6800 - val_loss: 0.7900 - val_accuracy: 0.5916
    Epoch 4/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.4699 - accuracy: 0.7315 - val_loss: 0.9969 - val_accuracy: 0.5507
    Epoch 5/20
    63/63 [==============================] - 7s 114ms/step - loss: 0.3978 - accuracy: 0.7850 - val_loss: 1.0592 - val_accuracy: 0.5668
    Epoch 6/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.3416 - accuracy: 0.8450 - val_loss: 1.2427 - val_accuracy: 0.5606
    Epoch 7/20
    63/63 [==============================] - 6s 98ms/step - loss: 0.2807 - accuracy: 0.8790 - val_loss: 1.2308 - val_accuracy: 0.5743
    Epoch 8/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.2491 - accuracy: 0.8925 - val_loss: 1.3877 - val_accuracy: 0.5644
    Epoch 9/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.2537 - accuracy: 0.8930 - val_loss: 1.4337 - val_accuracy: 0.5866
    Epoch 10/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.2140 - accuracy: 0.9095 - val_loss: 1.5328 - val_accuracy: 0.5804
    Epoch 11/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.2148 - accuracy: 0.9195 - val_loss: 1.7435 - val_accuracy: 0.5965
    Epoch 12/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.1902 - accuracy: 0.9220 - val_loss: 1.6347 - val_accuracy: 0.5767
    Epoch 13/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.1888 - accuracy: 0.9290 - val_loss: 1.5812 - val_accuracy: 0.5916
    Epoch 14/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.1423 - accuracy: 0.9465 - val_loss: 1.8671 - val_accuracy: 0.6064
    Epoch 15/20
    63/63 [==============================] - 6s 83ms/step - loss: 0.1824 - accuracy: 0.9375 - val_loss: 1.8601 - val_accuracy: 0.5545
    Epoch 16/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.2147 - accuracy: 0.9140 - val_loss: 2.0728 - val_accuracy: 0.5978
    Epoch 17/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.1605 - accuracy: 0.9575 - val_loss: 2.0723 - val_accuracy: 0.5705
    Epoch 18/20
    63/63 [==============================] - 5s 83ms/step - loss: 0.1201 - accuracy: 0.9580 - val_loss: 2.1988 - val_accuracy: 0.5941
    Epoch 19/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.1366 - accuracy: 0.9540 - val_loss: 2.4885 - val_accuracy: 0.5891
    Epoch 20/20
    63/63 [==============================] - 5s 83ms/step - loss: 0.1059 - accuracy: 0.9645 - val_loss: 2.4728 - val_accuracy: 0.6027
    


```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fcedd02f0d0>




![output_13_1.png]({{ site.baseurl }}/images/output_13_1.png)        
    


**From above we can see that the accuracy is between 55% and 60% compare to the baseline.** I improve the result about 5%. And there is a over fitting since the accuracy of the training data set is much more higher than the validation data set.

## **ยง3. Model with Data Augmentation** 

1. First, create a tf.keras.layers.RandomFlip() layer. Make a plot of the original image and a few copies to which RandomFlip() has been applied.


```python
data_RandomFlip = tf.keras.Sequential([
  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal')
])
for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    augmented_image = data_RandomFlip(tf.expand_dims(first_image, 0),training=True)
    plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
```


    
![output_17_0.png]({{ site.baseurl }}/images/output_17_0.png)  
    


2. Next, create a tf.keras.layers.RandomRotation() layer. Check the docs to learn more about the arguments accepted by this layer. Then, make a plot of both the original image and a few copies to which RandomRotation() has been applied.


```python
data_RandomRotation = tf.keras.Sequential([
  tf.keras.layers.experimental.preprocessing.RandomRotation(0.3)
])
for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    augmented_image = data_RandomRotation(tf.expand_dims(first_image, 0),training=True)
    plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
```


![output_19_0.png]({{ site.baseurl }}/images/output_19_0.png)     
    


Now, create a new tf.keras.models.Sequential model called model2 in which the first two layers are augmentation layers. Use a RandomFlip() layer and a RandomRotation() layer. 


```python
data_augmentation = tf.keras.Sequential([
  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),
  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),
])
```


```python
model2 = models.Sequential([
      data_augmentation,
      layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
      layers.MaxPooling2D((2, 2)),
      layers.Conv2D(32, (3, 3), activation='relu'),
      layers.MaxPooling2D((2, 2)),
      layers.Dropout(0.2),
      layers.Flatten(), # n^2 * 64 length vector
      
      layers.Dense(64, activation='relu'),
      layers.Dense(2) # number of classes in your dataset
])
```


```python
model2.compile(optimizer='adam', 
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics = ['accuracy'])
```


```python
history2 = model2.fit(train_dataset, 
           epochs=20, 
           validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 6s 81ms/step - loss: 73.2811 - accuracy: 0.5205 - val_loss: 0.6916 - val_accuracy: 0.5371
    Epoch 2/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6934 - accuracy: 0.5265 - val_loss: 0.6879 - val_accuracy: 0.5408
    Epoch 3/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6782 - accuracy: 0.5665 - val_loss: 0.6997 - val_accuracy: 0.5681
    Epoch 4/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6867 - accuracy: 0.5635 - val_loss: 0.6885 - val_accuracy: 0.5792
    Epoch 5/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6799 - accuracy: 0.5925 - val_loss: 0.6850 - val_accuracy: 0.5866
    Epoch 6/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6804 - accuracy: 0.5705 - val_loss: 0.6909 - val_accuracy: 0.5780
    Epoch 7/20
    63/63 [==============================] - 6s 97ms/step - loss: 0.6671 - accuracy: 0.6025 - val_loss: 0.7179 - val_accuracy: 0.5681
    Epoch 8/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6686 - accuracy: 0.5965 - val_loss: 0.6946 - val_accuracy: 0.5532
    Epoch 9/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6685 - accuracy: 0.6050 - val_loss: 0.6910 - val_accuracy: 0.5644
    Epoch 10/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.6609 - accuracy: 0.6040 - val_loss: 0.6921 - val_accuracy: 0.5866
    Epoch 11/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6624 - accuracy: 0.6170 - val_loss: 0.6935 - val_accuracy: 0.5619
    Epoch 12/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6612 - accuracy: 0.6115 - val_loss: 0.6887 - val_accuracy: 0.5953
    Epoch 13/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6602 - accuracy: 0.6135 - val_loss: 0.6855 - val_accuracy: 0.6002
    Epoch 14/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6512 - accuracy: 0.6290 - val_loss: 0.6941 - val_accuracy: 0.5891
    Epoch 15/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6445 - accuracy: 0.6365 - val_loss: 0.6757 - val_accuracy: 0.5965
    Epoch 16/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6472 - accuracy: 0.6245 - val_loss: 0.6782 - val_accuracy: 0.6163
    Epoch 17/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6302 - accuracy: 0.6405 - val_loss: 0.6945 - val_accuracy: 0.6126
    Epoch 18/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6376 - accuracy: 0.6325 - val_loss: 0.6865 - val_accuracy: 0.6151
    Epoch 19/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6351 - accuracy: 0.6415 - val_loss: 0.6776 - val_accuracy: 0.6101
    Epoch 20/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6381 - accuracy: 0.6280 - val_loss: 0.6725 - val_accuracy: 0.6002
    


```python
plt.plot(history2.history["accuracy"], label = "training")
plt.plot(history2.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fcedbaec210>




![output_25_1.png]({{ site.baseurl }}/images/output_25_1.png)     
    


**From above we can see that the accuracy is about 53%-60% at the end** ,which is little bit less than model1 this may cause by RandomFlip and RamdomRotation. There is still a overfitting but is getting better.

## **ยง4. Data Preprocessing**


```python
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])      
model3 = models.Sequential([
      preprocessor,
      data_augmentation,
      layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
      layers.MaxPooling2D((2, 2)),
      layers.Conv2D(32, (3, 3), activation='relu'),
      layers.MaxPooling2D((2, 2)),
      layers.Dropout(0.2),
      layers.Flatten(),  
      
      layers.Dense(64, activation='relu'),
      layers.Dense(2) # number of classes in your dataset
])          
```


```python
model3.compile(optimizer='adam', 
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics = ['accuracy'])
```


```python
history3 = model3.fit(train_dataset, 
           epochs=20, 
           validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 18s 88ms/step - loss: 0.7705 - accuracy: 0.5100 - val_loss: 0.6930 - val_accuracy: 0.5111
    Epoch 2/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6929 - accuracy: 0.4990 - val_loss: 0.6898 - val_accuracy: 0.5087
    Epoch 3/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6690 - accuracy: 0.5555 - val_loss: 0.6584 - val_accuracy: 0.5780
    Epoch 4/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6412 - accuracy: 0.5975 - val_loss: 0.6497 - val_accuracy: 0.6151
    Epoch 5/20
    63/63 [==============================] - 6s 96ms/step - loss: 0.6439 - accuracy: 0.5985 - val_loss: 0.6477 - val_accuracy: 0.6300
    Epoch 6/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.6309 - accuracy: 0.6165 - val_loss: 0.6354 - val_accuracy: 0.6213
    Epoch 7/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.6285 - accuracy: 0.6275 - val_loss: 0.6331 - val_accuracy: 0.6374
    Epoch 8/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.6110 - accuracy: 0.6455 - val_loss: 0.6237 - val_accuracy: 0.6436
    Epoch 9/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.6073 - accuracy: 0.6500 - val_loss: 0.6141 - val_accuracy: 0.6869
    Epoch 10/20
    63/63 [==============================] - 6s 83ms/step - loss: 0.6010 - accuracy: 0.6600 - val_loss: 0.6377 - val_accuracy: 0.6473
    Epoch 11/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.5867 - accuracy: 0.6810 - val_loss: 0.6349 - val_accuracy: 0.6559
    Epoch 12/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.5793 - accuracy: 0.6825 - val_loss: 0.6021 - val_accuracy: 0.6770
    Epoch 13/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.5663 - accuracy: 0.6910 - val_loss: 0.5990 - val_accuracy: 0.6832
    Epoch 14/20
    63/63 [==============================] - 7s 98ms/step - loss: 0.5713 - accuracy: 0.7065 - val_loss: 0.5791 - val_accuracy: 0.6980
    Epoch 15/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.5429 - accuracy: 0.7265 - val_loss: 0.6034 - val_accuracy: 0.6968
    Epoch 16/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.5418 - accuracy: 0.7215 - val_loss: 0.6488 - val_accuracy: 0.6634
    Epoch 17/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.5199 - accuracy: 0.7495 - val_loss: 0.6088 - val_accuracy: 0.6993
    Epoch 18/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.5171 - accuracy: 0.7375 - val_loss: 0.5677 - val_accuracy: 0.7302
    Epoch 19/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.5017 - accuracy: 0.7520 - val_loss: 0.5802 - val_accuracy: 0.7067
    Epoch 20/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.5115 - accuracy: 0.7515 - val_loss: 0.5611 - val_accuracy: 0.7339
    


```python
plt.plot(history3.history["accuracy"], label = "training")
plt.plot(history3.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fcedfa32390>




![output_31_1.png]({{ site.baseurl }}/images/output_31_1.png)      
    


In bold font, describe the validation accuracy of your model during training.
**From the graph we can see that the accuracy of validation is incresing is about 72% at the end.** Compare to model 1 we did imporve a lot, which is 10%!!!
And there is still an overfitting but is better then before!

## **ยง5. Transfer Learning**


```python
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])
```

    Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_160_no_top.h5
    9412608/9406464 [==============================] - 0s 0us/step
    9420800/9406464 [==============================] - 0s 0us/step
    


```python
model4 = models.Sequential([
      preprocessor,
      data_augmentation,
      base_model_layer,
      layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
      layers.MaxPooling2D((2, 2)),
      layers.Dropout(0.2),
      layers.Flatten(),  
      layers.Dense(2) # number of classes in your dataset
]) 
```


```python
model4.compile(optimizer='adam', 
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics = ['accuracy'])
```


```python
history4 = model4.fit(train_dataset, 
           epochs=20, 
           validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 10s 106ms/step - loss: 0.3052 - accuracy: 0.9055 - val_loss: 0.0892 - val_accuracy: 0.9678
    Epoch 2/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.1530 - accuracy: 0.9355 - val_loss: 0.0499 - val_accuracy: 0.9802
    Epoch 3/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.1312 - accuracy: 0.9430 - val_loss: 0.0539 - val_accuracy: 0.9814
    Epoch 4/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.1181 - accuracy: 0.9490 - val_loss: 0.0536 - val_accuracy: 0.9777
    Epoch 5/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.1072 - accuracy: 0.9500 - val_loss: 0.0384 - val_accuracy: 0.9814
    Epoch 6/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.0933 - accuracy: 0.9640 - val_loss: 0.0451 - val_accuracy: 0.9814
    Epoch 7/20
    63/63 [==============================] - 6s 93ms/step - loss: 0.0929 - accuracy: 0.9645 - val_loss: 0.0434 - val_accuracy: 0.9827
    Epoch 8/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.0956 - accuracy: 0.9585 - val_loss: 0.0466 - val_accuracy: 0.9777
    Epoch 9/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.0837 - accuracy: 0.9670 - val_loss: 0.0483 - val_accuracy: 0.9765
    Epoch 10/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.0774 - accuracy: 0.9670 - val_loss: 0.0433 - val_accuracy: 0.9814
    Epoch 11/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.0771 - accuracy: 0.9665 - val_loss: 0.0406 - val_accuracy: 0.9802
    Epoch 12/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.0848 - accuracy: 0.9680 - val_loss: 0.0510 - val_accuracy: 0.9827
    Epoch 13/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.0811 - accuracy: 0.9700 - val_loss: 0.0551 - val_accuracy: 0.9802
    Epoch 14/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.0745 - accuracy: 0.9710 - val_loss: 0.0410 - val_accuracy: 0.9777
    Epoch 15/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.0699 - accuracy: 0.9770 - val_loss: 0.0372 - val_accuracy: 0.9839
    Epoch 16/20
    63/63 [==============================] - 6s 93ms/step - loss: 0.0635 - accuracy: 0.9740 - val_loss: 0.0552 - val_accuracy: 0.9802
    Epoch 17/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.0684 - accuracy: 0.9700 - val_loss: 0.0454 - val_accuracy: 0.9765
    Epoch 18/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.0542 - accuracy: 0.9825 - val_loss: 0.0436 - val_accuracy: 0.9814
    Epoch 19/20
    63/63 [==============================] - 6s 94ms/step - loss: 0.0493 - accuracy: 0.9790 - val_loss: 0.0449 - val_accuracy: 0.9777
    Epoch 20/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.0397 - accuracy: 0.9875 - val_loss: 0.0492 - val_accuracy: 0.9814
    


```python
plt.plot(history4.history["accuracy"], label = "training")
plt.plot(history4.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fcedb416cd0>




![output_38_1.png]({{ site.baseurl }}/images/output_38_1.png)      
    



```python
model4.summary()
```

    Model: "sequential_15"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     model_1 (Functional)        (None, 160, 160, 3)       0         
                                                                     
     sequential_5 (Sequential)   (None, 160, 160, 3)       0         
                                                                     
     model_2 (Functional)        (None, 5, 5, 1280)        2257984   
                                                                     
     conv2d_20 (Conv2D)          (None, 3, 3, 32)          368672    
                                                                     
     max_pooling2d_19 (MaxPoolin  (None, 1, 1, 32)         0         
     g2D)                                                            
                                                                     
     dropout_10 (Dropout)        (None, 1, 1, 32)          0         
                                                                     
     flatten_11 (Flatten)        (None, 32)                0         
                                                                     
     dense_21 (Dense)            (None, 2)                 66        
                                                                     
    =================================================================
    Total params: 2,626,722
    Trainable params: 368,738
    Non-trainable params: 2,257,984
    _________________________________________________________________
    

The graph shows that we have Total params: 2,626,722Trainable params: 368,738Non-trainable params: 2,257,984, so we can see that our model is very complex!

**Our final accuracy is 96%-98% so our results are pretty good.** The accuracy has been greatly improved compared to the previous model. And through the image, there is no sign of over fit

## **ยง6. Score on Test Data**


```python
final = model4.fit(train_dataset, 
           epochs=20, 
           validation_data=test_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 5s 73ms/step - loss: 0.0422 - accuracy: 0.9855 - val_loss: 0.0336 - val_accuracy: 0.9896
    Epoch 2/20
    63/63 [==============================] - 5s 72ms/step - loss: 0.0386 - accuracy: 0.9865 - val_loss: 0.0674 - val_accuracy: 0.9844
    Epoch 3/20
    63/63 [==============================] - 5s 72ms/step - loss: 0.0345 - accuracy: 0.9885 - val_loss: 0.0453 - val_accuracy: 0.9948
    Epoch 4/20
    63/63 [==============================] - 5s 75ms/step - loss: 0.0184 - accuracy: 0.9925 - val_loss: 0.0612 - val_accuracy: 0.9896
    Epoch 5/20
    63/63 [==============================] - 5s 73ms/step - loss: 0.0470 - accuracy: 0.9825 - val_loss: 0.0431 - val_accuracy: 0.9740
    Epoch 6/20
    63/63 [==============================] - 5s 73ms/step - loss: 0.0384 - accuracy: 0.9840 - val_loss: 0.0412 - val_accuracy: 0.9948
    Epoch 7/20
    63/63 [==============================] - 5s 74ms/step - loss: 0.0459 - accuracy: 0.9840 - val_loss: 0.0047 - val_accuracy: 1.0000
    Epoch 8/20
    63/63 [==============================] - 5s 73ms/step - loss: 0.0325 - accuracy: 0.9875 - val_loss: 0.0907 - val_accuracy: 0.9792
    Epoch 9/20
    63/63 [==============================] - 5s 73ms/step - loss: 0.0340 - accuracy: 0.9895 - val_loss: 0.0308 - val_accuracy: 0.9948
    Epoch 10/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.0281 - accuracy: 0.9910 - val_loss: 0.0027 - val_accuracy: 1.0000
    Epoch 11/20
    63/63 [==============================] - 5s 73ms/step - loss: 0.0343 - accuracy: 0.9880 - val_loss: 0.0303 - val_accuracy: 0.9896
    Epoch 12/20
    63/63 [==============================] - 5s 74ms/step - loss: 0.0347 - accuracy: 0.9890 - val_loss: 0.0377 - val_accuracy: 0.9896
    Epoch 13/20
    63/63 [==============================] - 5s 73ms/step - loss: 0.0339 - accuracy: 0.9875 - val_loss: 0.0395 - val_accuracy: 0.9896
    Epoch 14/20
    63/63 [==============================] - 5s 73ms/step - loss: 0.0325 - accuracy: 0.9905 - val_loss: 0.0272 - val_accuracy: 0.9896
    Epoch 15/20
    63/63 [==============================] - 5s 73ms/step - loss: 0.0263 - accuracy: 0.9900 - val_loss: 0.1061 - val_accuracy: 0.9844
    Epoch 16/20
    63/63 [==============================] - 5s 73ms/step - loss: 0.0299 - accuracy: 0.9870 - val_loss: 0.0563 - val_accuracy: 0.9844
    Epoch 17/20
    63/63 [==============================] - 5s 72ms/step - loss: 0.0382 - accuracy: 0.9875 - val_loss: 0.0602 - val_accuracy: 0.9844
    Epoch 18/20
    63/63 [==============================] - 5s 73ms/step - loss: 0.0294 - accuracy: 0.9860 - val_loss: 0.0553 - val_accuracy: 0.9844
    Epoch 19/20
    63/63 [==============================] - 5s 72ms/step - loss: 0.0353 - accuracy: 0.9865 - val_loss: 0.0299 - val_accuracy: 0.9948
    Epoch 20/20
    63/63 [==============================] - 5s 72ms/step - loss: 0.0361 - accuracy: 0.9860 - val_loss: 0.0070 - val_accuracy: 0.9948
    


```python
plt.plot(final.history["accuracy"], label = "training")
plt.plot(final.history["val_accuracy"], label = "test")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fcedb177590>




![output_44_1.png]({{ site.baseurl }}/images/output_44_1.png)      
    


**Our accuracy is about 100%!!!! and there is no over fitting!**
